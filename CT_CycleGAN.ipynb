{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNjDKdQy35h"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRm-USlsHgEV",
        "outputId": "15b123f3-b446-46ca-bb3d-eaff04b476d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2447, done.\u001b[K\n",
            "remote: Total 2447 (delta 0), reused 0 (delta 0), pack-reused 2447\u001b[K\n",
            "Receiving objects: 100% (2447/2447), 8.18 MiB | 13.53 MiB/s, done.\n",
            "Resolving deltas: 100% (1535/1535), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Vedansh-dixit/Cycle-GAN-Pytorch-ct-mri-translation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "20QffS41Nkno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pt3igws3eiVp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1EySlOXwwoa",
        "outputId": "0d6dedf7-023d-4367-8e08-97629db97123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.12.0+cu113)\n",
            "Collecting dominate>=2.4.0\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visdom>=0.1.8.8\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (22.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2.10)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Building wheels for collected packages: visdom, pathtools, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=e3d29e1305dcb14c31dfc8af8f25fcfca697a5396edc17de0c89dcc43a9c5525\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=58985fb65b0d3bf7125d16de3f2dc3137b629e47777cef35621b4049d70ef899\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=2328bab3785711622e8c61bbff7f8239effccc24380b7ab326f08bd8faec3106\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom pathtools torchfile\n",
            "Installing collected packages: smmap, jsonpointer, gitdb, websocket-client, torchfile, shortuuid, setproctitle, sentry-sdk, pathtools, jsonpatch, GitPython, docker-pycreds, wandb, visdom, dominate\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 dominate-2.6.0 gitdb-4.0.9 jsonpatch-1.32 jsonpointer-2.3 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 torchfile-0.1.0 visdom-0.1.8.9 wandb-0.12.16 websocket-client-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daqlgVhw29P"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "\n",
        "\n",
        "Use your own dataset by creating the appropriate folders and adding in the images.\n",
        "\n",
        "-   Create a dataset folder under `/dataset` for your dataset.\n",
        "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrdOettJxaCc",
        "outputId": "ca5e1363-36e9-4911-a2f4-63b35cd5bf52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-CycleGAN-and-pix2pix/datasets\n"
          ]
        }
      ],
      "source": [
        "%cd /content/pytorch-CycleGAN-and-pix2pix/datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir dataset"
      ],
      "metadata": {
        "id": "TOa5OYqtLKrZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-CycleGAN-and-pix2pix/datasets/dataset"
      ],
      "metadata": {
        "id": "Ab0_DCdzLQd4",
        "outputId": "404b2c53-50c7-4cd2-d13c-fed52679761a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-CycleGAN-and-pix2pix/datasets/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir testA\n",
        "%mkdir testB\n",
        "%mkdir trainA\n",
        "%mkdir trainB"
      ],
      "metadata": {
        "id": "sf5QT6vnLbg4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload data in th above directories"
      ],
      "metadata": {
        "id": "WhFESLGzPEzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "id": "a8UGwMmQLxVQ",
        "outputId": "b39a669c-40cf-4fb4-fcf6-2a1734875cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-CycleGAN-and-pix2pix/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xv57EyKjP_Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw1kDQBx3LN"
      },
      "source": [
        "# Training\n",
        "\n",
        "-   `python train.py --dataroot ./datasets/(name of dataset uploaded) --name (name of model) --model cycle_gan`\n",
        "\n",
        "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
        "\n",
        "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sp7TCT2x9dB",
        "outputId": "aff57105-9a05-4c76-aea3-b3fafda7b4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "           continue_train: False                         \n",
            "                crop_size: 256                           \n",
            "                 dataroot: ./datasets/dataset/           \t[default: None]\n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                 lambda_A: 10.0                          \n",
            "                 lambda_B: 10.0                          \n",
            "          lambda_identity: 0.5                           \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cycle_gan                     \n",
            "                 n_epochs: 100                           \n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: cttomri                       \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: instance                      \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 50                            \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: True                          \t[default: False]\n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "dataset [UnalignedDataset] was created\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "The number of training images = 48\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [CycleGANModel] was created\n",
            "---------- Networks initialized -------------\n",
            "[Network G_A] Total number of parameters : 11.378 M\n",
            "[Network G_B] Total number of parameters : 11.378 M\n",
            "[Network D_A] Total number of parameters : 2.765 M\n",
            "[Network D_B] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "Setting up a new session...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mv3dhawk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/pytorch-CycleGAN-and-pix2pix/wandb/run-20220511_130256-37yme9h7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcttomri\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/v3dhawk/CycleGAN-and-pix2pix\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/v3dhawk/CycleGAN-and-pix2pix/runs/37yme9h7\u001b[0m\n",
            "create web directory ./checkpoints/cttomri/web...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 1 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 2 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 3, iters: 4, time: 1.394, data: 0.231) D_A: 0.374 G_A: 0.315 cycle_A: 1.154 idt_A: 0.345 D_B: 0.263 G_B: 0.290 cycle_B: 0.792 idt_B: 0.497 \n",
            "End of epoch 3 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 4 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 5, iters: 8, time: 1.401, data: 0.005) D_A: 0.271 G_A: 0.431 cycle_A: 0.163 idt_A: 0.358 D_B: 0.282 G_B: 0.279 cycle_B: 0.868 idt_B: 0.070 \n",
            "saving the model at the end of epoch 5, iters 240\n",
            "End of epoch 5 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 6 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 7, iters: 12, time: 1.412, data: 0.001) D_A: 0.264 G_A: 0.253 cycle_A: 0.357 idt_A: 0.317 D_B: 0.259 G_B: 0.242 cycle_B: 0.791 idt_B: 0.134 \n",
            "End of epoch 7 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 8 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 9, iters: 16, time: 2.192, data: 0.003) D_A: 0.262 G_A: 0.268 cycle_A: 0.544 idt_A: 0.157 D_B: 0.259 G_B: 0.342 cycle_B: 0.355 idt_B: 0.236 \n",
            "End of epoch 9 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 10, iters 480\n",
            "End of epoch 10 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 11, iters: 20, time: 1.395, data: 0.007) D_A: 0.296 G_A: 0.259 cycle_A: 0.931 idt_A: 0.111 D_B: 0.257 G_B: 0.270 cycle_B: 0.248 idt_B: 0.390 \n",
            "End of epoch 11 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 12 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 13, iters: 24, time: 1.395, data: 0.004) D_A: 0.275 G_A: 0.387 cycle_A: 0.793 idt_A: 0.312 D_B: 0.255 G_B: 0.253 cycle_B: 0.726 idt_B: 0.330 \n",
            "End of epoch 13 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 14 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 15, iters: 28, time: 1.402, data: 0.004) D_A: 0.322 G_A: 0.276 cycle_A: 0.438 idt_A: 0.228 D_B: 0.295 G_B: 0.369 cycle_B: 0.565 idt_B: 0.171 \n",
            "saving the model at the end of epoch 15, iters 720\n",
            "End of epoch 15 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 16 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 17, iters: 32, time: 2.155, data: 0.004) D_A: 0.273 G_A: 0.273 cycle_A: 0.411 idt_A: 0.186 D_B: 0.268 G_B: 0.326 cycle_B: 0.457 idt_B: 0.181 \n",
            "End of epoch 17 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 18 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 19, iters: 36, time: 1.393, data: 0.003) D_A: 0.301 G_A: 0.318 cycle_A: 0.398 idt_A: 0.088 D_B: 0.264 G_B: 0.280 cycle_B: 0.203 idt_B: 0.167 \n",
            "End of epoch 19 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 20, iters 960\n",
            "End of epoch 20 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 21, iters: 40, time: 1.387, data: 0.004) D_A: 0.342 G_A: 0.396 cycle_A: 0.390 idt_A: 0.112 D_B: 0.354 G_B: 0.392 cycle_B: 0.264 idt_B: 0.162 \n",
            "End of epoch 21 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 22 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 23, iters: 44, time: 1.392, data: 0.004) D_A: 0.330 G_A: 0.354 cycle_A: 0.447 idt_A: 0.079 D_B: 0.241 G_B: 0.279 cycle_B: 0.193 idt_B: 0.195 \n",
            "End of epoch 23 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 24 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 25, iters: 48, time: 2.399, data: 0.004) D_A: 0.301 G_A: 0.274 cycle_A: 0.288 idt_A: 0.094 D_B: 0.271 G_B: 0.337 cycle_B: 0.242 idt_B: 0.112 \n",
            "saving the model at the end of epoch 25, iters 1200\n",
            "End of epoch 25 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 26 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 27 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 28, iters: 4, time: 1.398, data: 0.209) D_A: 0.297 G_A: 0.320 cycle_A: 0.501 idt_A: 0.230 D_B: 0.327 G_B: 0.288 cycle_B: 0.586 idt_B: 0.190 \n",
            "End of epoch 28 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 29 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 30, iters: 8, time: 1.399, data: 0.004) D_A: 0.280 G_A: 0.268 cycle_A: 0.379 idt_A: 0.112 D_B: 0.331 G_B: 0.226 cycle_B: 0.248 idt_B: 0.163 \n",
            "saving the model at the end of epoch 30, iters 1440\n",
            "End of epoch 30 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 31 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 32, iters: 12, time: 1.392, data: 0.004) D_A: 0.259 G_A: 0.242 cycle_A: 0.232 idt_A: 0.035 D_B: 0.252 G_B: 0.224 cycle_B: 0.088 idt_B: 0.103 \n",
            "End of epoch 32 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 33 / 200 \t Time Taken: 63 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 34, iters: 16, time: 2.545, data: 0.003) D_A: 0.266 G_A: 0.265 cycle_A: 0.152 idt_A: 0.083 D_B: 0.259 G_B: 0.274 cycle_B: 0.189 idt_B: 0.062 \n",
            "End of epoch 34 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 35, iters 1680\n",
            "End of epoch 35 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 36, iters: 20, time: 1.423, data: 0.007) D_A: 0.278 G_A: 0.187 cycle_A: 0.177 idt_A: 0.081 D_B: 0.345 G_B: 0.287 cycle_B: 0.194 idt_B: 0.066 \n",
            "End of epoch 36 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 37 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 38, iters: 24, time: 1.433, data: 0.007) D_A: 0.249 G_A: 0.393 cycle_A: 0.456 idt_A: 0.141 D_B: 0.322 G_B: 0.271 cycle_B: 0.323 idt_B: 0.182 \n",
            "End of epoch 38 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 39 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 40, iters: 28, time: 1.434, data: 0.011) D_A: 0.249 G_A: 0.262 cycle_A: 0.362 idt_A: 0.123 D_B: 0.271 G_B: 0.280 cycle_B: 0.333 idt_B: 0.130 \n",
            "saving the model at the end of epoch 40, iters 1920\n",
            "End of epoch 40 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 41 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 42, iters: 32, time: 2.512, data: 0.004) D_A: 0.432 G_A: 0.576 cycle_A: 0.292 idt_A: 0.121 D_B: 0.274 G_B: 0.249 cycle_B: 0.274 idt_B: 0.123 \n",
            "End of epoch 42 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 43 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 44, iters: 36, time: 1.416, data: 0.003) D_A: 0.253 G_A: 0.247 cycle_A: 0.170 idt_A: 0.108 D_B: 0.281 G_B: 0.264 cycle_B: 0.266 idt_B: 0.076 \n",
            "End of epoch 44 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 45, iters 2160\n",
            "End of epoch 45 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 46, iters: 40, time: 1.430, data: 0.004) D_A: 0.260 G_A: 0.185 cycle_A: 0.268 idt_A: 0.082 D_B: 0.336 G_B: 0.240 cycle_B: 0.192 idt_B: 0.102 \n",
            "End of epoch 46 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 47 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 48, iters: 44, time: 1.426, data: 0.003) D_A: 0.316 G_A: 0.372 cycle_A: 0.376 idt_A: 0.114 D_B: 0.269 G_B: 0.261 cycle_B: 0.273 idt_B: 0.153 \n",
            "End of epoch 48 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 49 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 50, iters: 48, time: 2.610, data: 0.003) D_A: 0.269 G_A: 0.260 cycle_A: 0.190 idt_A: 0.127 D_B: 0.282 G_B: 0.288 cycle_B: 0.367 idt_B: 0.078 \n",
            "saving the model at the end of epoch 50, iters 2400\n",
            "End of epoch 50 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 51 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 52 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 53, iters: 4, time: 1.430, data: 0.220) D_A: 0.318 G_A: 0.307 cycle_A: 0.545 idt_A: 0.032 D_B: 0.238 G_B: 0.312 cycle_B: 0.084 idt_B: 0.216 \n",
            "End of epoch 53 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 54 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 55, iters: 8, time: 1.420, data: 0.003) D_A: 0.270 G_A: 0.222 cycle_A: 0.156 idt_A: 0.070 D_B: 0.344 G_B: 0.341 cycle_B: 0.149 idt_B: 0.062 \n",
            "saving the model at the end of epoch 55, iters 2640\n",
            "End of epoch 55 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 56 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 57, iters: 12, time: 1.423, data: 0.001) D_A: 0.256 G_A: 0.165 cycle_A: 0.119 idt_A: 0.056 D_B: 0.324 G_B: 0.336 cycle_B: 0.149 idt_B: 0.052 \n",
            "End of epoch 57 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 58 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 59, iters: 16, time: 2.578, data: 0.004) D_A: 0.287 G_A: 0.329 cycle_A: 0.300 idt_A: 0.123 D_B: 0.271 G_B: 0.188 cycle_B: 0.310 idt_B: 0.130 \n",
            "End of epoch 59 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 60, iters 2880\n",
            "End of epoch 60 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 61, iters: 20, time: 1.431, data: 0.004) D_A: 0.307 G_A: 0.338 cycle_A: 0.118 idt_A: 0.152 D_B: 0.275 G_B: 0.344 cycle_B: 0.377 idt_B: 0.044 \n",
            "End of epoch 61 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 62 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 63, iters: 24, time: 1.421, data: 0.004) D_A: 0.247 G_A: 0.241 cycle_A: 0.601 idt_A: 0.766 D_B: 0.273 G_B: 0.285 cycle_B: 1.649 idt_B: 0.207 \n",
            "End of epoch 63 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 64 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 65, iters: 28, time: 1.417, data: 0.006) D_A: 0.268 G_A: 0.297 cycle_A: 0.691 idt_A: 0.166 D_B: 0.300 G_B: 0.309 cycle_B: 0.423 idt_B: 0.251 \n",
            "saving the model at the end of epoch 65, iters 3120\n",
            "End of epoch 65 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 66 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 67, iters: 32, time: 2.763, data: 0.004) D_A: 0.273 G_A: 0.235 cycle_A: 0.496 idt_A: 0.116 D_B: 0.282 G_B: 0.248 cycle_B: 0.293 idt_B: 0.199 \n",
            "End of epoch 67 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 68 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 69, iters: 36, time: 1.430, data: 0.006) D_A: 0.276 G_A: 0.270 cycle_A: 0.301 idt_A: 0.128 D_B: 0.322 G_B: 0.372 cycle_B: 0.289 idt_B: 0.119 \n",
            "End of epoch 69 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 70, iters 3360\n",
            "End of epoch 70 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 71, iters: 40, time: 1.418, data: 0.004) D_A: 0.290 G_A: 0.243 cycle_A: 0.368 idt_A: 0.475 D_B: 0.277 G_B: 0.276 cycle_B: 0.995 idt_B: 0.155 \n",
            "End of epoch 71 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 72 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 73, iters: 44, time: 1.427, data: 0.004) D_A: 0.253 G_A: 0.241 cycle_A: 0.428 idt_A: 0.132 D_B: 0.251 G_B: 0.284 cycle_B: 0.379 idt_B: 0.184 \n",
            "End of epoch 73 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 74 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 75, iters: 48, time: 2.947, data: 0.003) D_A: 0.245 G_A: 0.237 cycle_A: 0.444 idt_A: 0.194 D_B: 0.401 G_B: 0.378 cycle_B: 0.475 idt_B: 0.172 \n",
            "saving the model at the end of epoch 75, iters 3600\n",
            "End of epoch 75 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 76 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 77 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 78, iters: 4, time: 1.427, data: 0.162) D_A: 0.271 G_A: 0.224 cycle_A: 0.143 idt_A: 0.155 D_B: 0.257 G_B: 0.349 cycle_B: 0.357 idt_B: 0.064 \n",
            "End of epoch 78 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 79 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 80, iters: 8, time: 1.422, data: 0.001) D_A: 0.308 G_A: 0.250 cycle_A: 0.387 idt_A: 0.048 D_B: 0.246 G_B: 0.290 cycle_B: 0.106 idt_B: 0.162 \n",
            "saving the model at the end of epoch 80, iters 3840\n",
            "End of epoch 80 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 81 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 82, iters: 12, time: 1.425, data: 0.004) D_A: 0.384 G_A: 0.389 cycle_A: 0.422 idt_A: 0.099 D_B: 0.260 G_B: 0.216 cycle_B: 0.233 idt_B: 0.163 \n",
            "End of epoch 82 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 83 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 84, iters: 16, time: 2.724, data: 0.006) D_A: 0.283 G_A: 0.197 cycle_A: 0.284 idt_A: 0.170 D_B: 0.253 G_B: 0.280 cycle_B: 0.348 idt_B: 0.128 \n",
            "End of epoch 84 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 85, iters 4080\n",
            "End of epoch 85 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 86, iters: 20, time: 1.426, data: 0.003) D_A: 0.249 G_A: 0.257 cycle_A: 0.262 idt_A: 0.036 D_B: 0.247 G_B: 0.227 cycle_B: 0.083 idt_B: 0.099 \n",
            "End of epoch 86 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 87 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 88, iters: 24, time: 1.434, data: 0.003) D_A: 0.309 G_A: 0.330 cycle_A: 0.285 idt_A: 0.133 D_B: 0.272 G_B: 0.210 cycle_B: 0.343 idt_B: 0.119 \n",
            "End of epoch 88 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 89 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 90, iters: 28, time: 1.433, data: 0.004) D_A: 0.247 G_A: 0.215 cycle_A: 0.296 idt_A: 0.118 D_B: 0.229 G_B: 0.283 cycle_B: 0.362 idt_B: 0.119 \n",
            "saving the model at the end of epoch 90, iters 4320\n",
            "End of epoch 90 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 91 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 92, iters: 32, time: 3.645, data: 0.005) D_A: 0.258 G_A: 0.232 cycle_A: 0.257 idt_A: 0.093 D_B: 0.254 G_B: 0.281 cycle_B: 0.232 idt_B: 0.112 \n",
            "End of epoch 92 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 93 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 94, iters: 36, time: 1.433, data: 0.003) D_A: 0.261 G_A: 0.256 cycle_A: 0.242 idt_A: 0.079 D_B: 0.361 G_B: 0.429 cycle_B: 0.177 idt_B: 0.092 \n",
            "End of epoch 94 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "saving the model at the end of epoch 95, iters 4560\n",
            "End of epoch 95 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 96, iters: 40, time: 1.420, data: 0.004) D_A: 0.263 G_A: 0.275 cycle_A: 0.323 idt_A: 0.147 D_B: 0.269 G_B: 0.271 cycle_B: 0.367 idt_B: 0.105 \n",
            "End of epoch 96 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 97 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 98, iters: 44, time: 1.426, data: 0.004) D_A: 0.282 G_A: 0.246 cycle_A: 0.110 idt_A: 0.102 D_B: 0.259 G_B: 0.249 cycle_B: 0.283 idt_B: 0.046 \n",
            "End of epoch 98 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "End of epoch 99 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0001980\n",
            "(epoch: 100, iters: 48, time: 2.914, data: 0.012) D_A: 0.287 G_A: 0.191 cycle_A: 0.115 idt_A: 0.066 D_B: 0.251 G_B: 0.252 cycle_B: 0.148 idt_B: 0.050 \n",
            "saving the model at the end of epoch 100, iters 4800\n",
            "End of epoch 100 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001980 -> 0.0001960\n",
            "End of epoch 101 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001960 -> 0.0001941\n",
            "End of epoch 102 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001941 -> 0.0001921\n",
            "(epoch: 103, iters: 4, time: 1.427, data: 0.226) D_A: 0.245 G_A: 0.268 cycle_A: 0.297 idt_A: 0.141 D_B: 0.246 G_B: 0.251 cycle_B: 0.382 idt_B: 0.121 \n",
            "End of epoch 103 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001921 -> 0.0001901\n",
            "End of epoch 104 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001901 -> 0.0001881\n",
            "(epoch: 105, iters: 8, time: 1.427, data: 0.004) D_A: 0.256 G_A: 0.215 cycle_A: 0.179 idt_A: 0.089 D_B: 0.252 G_B: 0.268 cycle_B: 0.217 idt_B: 0.065 \n",
            "saving the latest model (epoch 105, total_iters 5000)\n",
            "saving the model at the end of epoch 105, iters 5040\n",
            "End of epoch 105 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001881 -> 0.0001861\n",
            "End of epoch 106 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001861 -> 0.0001842\n",
            "(epoch: 107, iters: 12, time: 1.427, data: 0.001) D_A: 0.247 G_A: 0.277 cycle_A: 0.223 idt_A: 0.088 D_B: 0.242 G_B: 0.256 cycle_B: 0.220 idt_B: 0.094 \n",
            "End of epoch 107 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001842 -> 0.0001822\n",
            "End of epoch 108 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001822 -> 0.0001802\n",
            "(epoch: 109, iters: 16, time: 3.083, data: 0.003) D_A: 0.263 G_A: 0.314 cycle_A: 0.158 idt_A: 0.079 D_B: 0.247 G_B: 0.244 cycle_B: 0.182 idt_B: 0.071 \n",
            "End of epoch 109 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001802 -> 0.0001782\n",
            "saving the model at the end of epoch 110, iters 5280\n",
            "End of epoch 110 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001782 -> 0.0001762\n",
            "(epoch: 111, iters: 20, time: 1.423, data: 0.003) D_A: 0.255 G_A: 0.240 cycle_A: 0.304 idt_A: 0.116 D_B: 0.254 G_B: 0.289 cycle_B: 0.299 idt_B: 0.135 \n",
            "End of epoch 111 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001762 -> 0.0001743\n",
            "End of epoch 112 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001743 -> 0.0001723\n",
            "(epoch: 113, iters: 24, time: 1.420, data: 0.007) D_A: 0.273 G_A: 0.251 cycle_A: 0.286 idt_A: 0.153 D_B: 0.269 G_B: 0.215 cycle_B: 0.371 idt_B: 0.116 \n",
            "End of epoch 113 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001723 -> 0.0001703\n",
            "End of epoch 114 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001703 -> 0.0001683\n",
            "(epoch: 115, iters: 28, time: 1.427, data: 0.003) D_A: 0.249 G_A: 0.281 cycle_A: 0.166 idt_A: 0.035 D_B: 0.270 G_B: 0.244 cycle_B: 0.078 idt_B: 0.075 \n",
            "saving the model at the end of epoch 115, iters 5520\n",
            "End of epoch 115 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001683 -> 0.0001663\n",
            "End of epoch 116 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001663 -> 0.0001644\n",
            "(epoch: 117, iters: 32, time: 3.170, data: 0.003) D_A: 0.255 G_A: 0.237 cycle_A: 0.212 idt_A: 0.082 D_B: 0.273 G_B: 0.258 cycle_B: 0.214 idt_B: 0.095 \n",
            "End of epoch 117 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001644 -> 0.0001624\n",
            "End of epoch 118 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001624 -> 0.0001604\n",
            "(epoch: 119, iters: 36, time: 1.434, data: 0.005) D_A: 0.239 G_A: 0.256 cycle_A: 0.137 idt_A: 0.086 D_B: 0.255 G_B: 0.275 cycle_B: 0.212 idt_B: 0.066 \n",
            "End of epoch 119 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001604 -> 0.0001584\n",
            "saving the model at the end of epoch 120, iters 5760\n",
            "End of epoch 120 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001584 -> 0.0001564\n",
            "(epoch: 121, iters: 40, time: 1.439, data: 0.004) D_A: 0.264 G_A: 0.235 cycle_A: 0.139 idt_A: 0.137 D_B: 0.253 G_B: 0.310 cycle_B: 0.309 idt_B: 0.058 \n",
            "End of epoch 121 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001564 -> 0.0001545\n",
            "End of epoch 122 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001545 -> 0.0001525\n",
            "(epoch: 123, iters: 44, time: 1.439, data: 0.004) D_A: 0.265 G_A: 0.252 cycle_A: 0.193 idt_A: 0.101 D_B: 0.242 G_B: 0.265 cycle_B: 0.255 idt_B: 0.085 \n",
            "End of epoch 123 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001525 -> 0.0001505\n",
            "End of epoch 124 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001505 -> 0.0001485\n",
            "(epoch: 125, iters: 48, time: 3.192, data: 0.003) D_A: 0.284 G_A: 0.230 cycle_A: 0.188 idt_A: 0.083 D_B: 0.271 G_B: 0.201 cycle_B: 0.237 idt_B: 0.080 \n",
            "saving the model at the end of epoch 125, iters 6000\n",
            "End of epoch 125 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001485 -> 0.0001465\n",
            "End of epoch 126 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001465 -> 0.0001446\n",
            "End of epoch 127 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001446 -> 0.0001426\n",
            "(epoch: 128, iters: 4, time: 1.444, data: 0.171) D_A: 0.270 G_A: 0.218 cycle_A: 0.256 idt_A: 0.030 D_B: 0.263 G_B: 0.230 cycle_B: 0.068 idt_B: 0.096 \n",
            "End of epoch 128 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001426 -> 0.0001406\n",
            "End of epoch 129 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001406 -> 0.0001386\n",
            "(epoch: 130, iters: 8, time: 1.435, data: 0.005) D_A: 0.254 G_A: 0.279 cycle_A: 0.271 idt_A: 0.065 D_B: 0.243 G_B: 0.282 cycle_B: 0.139 idt_B: 0.111 \n",
            "saving the model at the end of epoch 130, iters 6240\n",
            "End of epoch 130 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001386 -> 0.0001366\n",
            "End of epoch 131 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001366 -> 0.0001347\n",
            "(epoch: 132, iters: 12, time: 1.434, data: 0.003) D_A: 0.265 G_A: 0.240 cycle_A: 0.173 idt_A: 0.086 D_B: 0.262 G_B: 0.314 cycle_B: 0.201 idt_B: 0.070 \n",
            "End of epoch 132 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001347 -> 0.0001327\n",
            "End of epoch 133 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001327 -> 0.0001307\n",
            "(epoch: 134, iters: 16, time: 3.213, data: 0.003) D_A: 0.268 G_A: 0.273 cycle_A: 0.201 idt_A: 0.051 D_B: 0.254 G_B: 0.258 cycle_B: 0.116 idt_B: 0.081 \n",
            "End of epoch 134 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001307 -> 0.0001287\n",
            "saving the model at the end of epoch 135, iters 6480\n",
            "End of epoch 135 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001287 -> 0.0001267\n",
            "(epoch: 136, iters: 20, time: 1.432, data: 0.003) D_A: 0.268 G_A: 0.265 cycle_A: 0.246 idt_A: 0.065 D_B: 0.268 G_B: 0.190 cycle_B: 0.166 idt_B: 0.098 \n",
            "End of epoch 136 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001267 -> 0.0001248\n",
            "End of epoch 137 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001248 -> 0.0001228\n",
            "(epoch: 138, iters: 24, time: 1.420, data: 0.003) D_A: 0.258 G_A: 0.233 cycle_A: 0.152 idt_A: 0.087 D_B: 0.256 G_B: 0.285 cycle_B: 0.223 idt_B: 0.062 \n",
            "End of epoch 138 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001228 -> 0.0001208\n",
            "End of epoch 139 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001208 -> 0.0001188\n",
            "(epoch: 140, iters: 28, time: 1.430, data: 0.003) D_A: 0.243 G_A: 0.235 cycle_A: 0.073 idt_A: 0.080 D_B: 0.242 G_B: 0.295 cycle_B: 0.189 idt_B: 0.030 \n",
            "saving the model at the end of epoch 140, iters 6720\n",
            "End of epoch 140 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001188 -> 0.0001168\n",
            "End of epoch 141 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001168 -> 0.0001149\n",
            "(epoch: 142, iters: 32, time: 3.232, data: 0.008) D_A: 0.253 G_A: 0.288 cycle_A: 0.069 idt_A: 0.086 D_B: 0.244 G_B: 0.250 cycle_B: 0.180 idt_B: 0.031 \n",
            "End of epoch 142 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001149 -> 0.0001129\n",
            "End of epoch 143 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001129 -> 0.0001109\n",
            "(epoch: 144, iters: 36, time: 1.432, data: 0.003) D_A: 0.253 G_A: 0.256 cycle_A: 0.218 idt_A: 0.021 D_B: 0.255 G_B: 0.247 cycle_B: 0.044 idt_B: 0.092 \n",
            "End of epoch 144 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001109 -> 0.0001089\n",
            "saving the model at the end of epoch 145, iters 6960\n",
            "End of epoch 145 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001089 -> 0.0001069\n",
            "(epoch: 146, iters: 40, time: 1.427, data: 0.005) D_A: 0.254 G_A: 0.252 cycle_A: 0.077 idt_A: 0.118 D_B: 0.258 G_B: 0.266 cycle_B: 0.261 idt_B: 0.034 \n",
            "End of epoch 146 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001069 -> 0.0001050\n",
            "End of epoch 147 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001050 -> 0.0001030\n",
            "(epoch: 148, iters: 44, time: 1.440, data: 0.004) D_A: 0.247 G_A: 0.280 cycle_A: 0.159 idt_A: 0.078 D_B: 0.260 G_B: 0.231 cycle_B: 0.182 idt_B: 0.069 \n",
            "End of epoch 148 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0001030 -> 0.0001010\n",
            "End of epoch 149 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0001010 -> 0.0000990\n",
            "(epoch: 150, iters: 48, time: 3.544, data: 0.003) D_A: 0.259 G_A: 0.266 cycle_A: 0.164 idt_A: 0.100 D_B: 0.270 G_B: 0.236 cycle_B: 0.192 idt_B: 0.072 \n",
            "saving the model at the end of epoch 150, iters 7200\n",
            "End of epoch 150 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000990 -> 0.0000970\n",
            "End of epoch 151 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000970 -> 0.0000950\n",
            "End of epoch 152 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000950 -> 0.0000931\n",
            "(epoch: 153, iters: 4, time: 1.429, data: 0.219) D_A: 0.248 G_A: 0.280 cycle_A: 0.200 idt_A: 0.075 D_B: 0.249 G_B: 0.244 cycle_B: 0.174 idt_B: 0.088 \n",
            "End of epoch 153 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000931 -> 0.0000911\n",
            "End of epoch 154 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000911 -> 0.0000891\n",
            "(epoch: 155, iters: 8, time: 1.433, data: 0.006) D_A: 0.226 G_A: 0.289 cycle_A: 0.032 idt_A: 0.093 D_B: 0.270 G_B: 0.232 cycle_B: 0.216 idt_B: 0.016 \n",
            "saving the model at the end of epoch 155, iters 7440\n",
            "End of epoch 155 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000891 -> 0.0000871\n",
            "End of epoch 156 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000871 -> 0.0000851\n",
            "(epoch: 157, iters: 12, time: 1.425, data: 0.004) D_A: 0.238 G_A: 0.272 cycle_A: 0.112 idt_A: 0.068 D_B: 0.243 G_B: 0.289 cycle_B: 0.151 idt_B: 0.051 \n",
            "End of epoch 157 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000851 -> 0.0000832\n",
            "End of epoch 158 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000832 -> 0.0000812\n",
            "(epoch: 159, iters: 16, time: 3.647, data: 0.004) D_A: 0.259 G_A: 0.277 cycle_A: 0.229 idt_A: 0.064 D_B: 0.246 G_B: 0.287 cycle_B: 0.148 idt_B: 0.094 \n",
            "End of epoch 159 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000812 -> 0.0000792\n",
            "saving the model at the end of epoch 160, iters 7680\n",
            "End of epoch 160 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000792 -> 0.0000772\n",
            "(epoch: 161, iters: 20, time: 1.431, data: 0.005) D_A: 0.254 G_A: 0.234 cycle_A: 0.204 idt_A: 0.046 D_B: 0.258 G_B: 0.264 cycle_B: 0.101 idt_B: 0.090 \n",
            "End of epoch 161 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000772 -> 0.0000752\n",
            "End of epoch 162 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000752 -> 0.0000733\n",
            "(epoch: 163, iters: 24, time: 1.430, data: 0.003) D_A: 0.254 G_A: 0.260 cycle_A: 0.101 idt_A: 0.038 D_B: 0.252 G_B: 0.254 cycle_B: 0.085 idt_B: 0.044 \n",
            "End of epoch 163 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000733 -> 0.0000713\n",
            "End of epoch 164 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000713 -> 0.0000693\n",
            "(epoch: 165, iters: 28, time: 1.438, data: 0.003) D_A: 0.249 G_A: 0.260 cycle_A: 0.090 idt_A: 0.028 D_B: 0.263 G_B: 0.211 cycle_B: 0.061 idt_B: 0.040 \n",
            "saving the model at the end of epoch 165, iters 7920\n",
            "End of epoch 165 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000693 -> 0.0000673\n",
            "End of epoch 166 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000673 -> 0.0000653\n",
            "(epoch: 167, iters: 32, time: 3.581, data: 0.004) D_A: 0.249 G_A: 0.248 cycle_A: 0.224 idt_A: 0.055 D_B: 0.247 G_B: 0.237 cycle_B: 0.122 idt_B: 0.093 \n",
            "End of epoch 167 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000653 -> 0.0000634\n",
            "End of epoch 168 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000634 -> 0.0000614\n",
            "(epoch: 169, iters: 36, time: 1.438, data: 0.003) D_A: 0.264 G_A: 0.236 cycle_A: 0.034 idt_A: 0.040 D_B: 0.246 G_B: 0.245 cycle_B: 0.090 idt_B: 0.016 \n",
            "End of epoch 169 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000614 -> 0.0000594\n",
            "saving the model at the end of epoch 170, iters 8160\n",
            "End of epoch 170 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000594 -> 0.0000574\n",
            "(epoch: 171, iters: 40, time: 1.430, data: 0.007) D_A: 0.241 G_A: 0.234 cycle_A: 0.142 idt_A: 0.017 D_B: 0.265 G_B: 0.252 cycle_B: 0.036 idt_B: 0.058 \n",
            "End of epoch 171 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000574 -> 0.0000554\n",
            "End of epoch 172 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000554 -> 0.0000535\n",
            "(epoch: 173, iters: 44, time: 1.434, data: 0.003) D_A: 0.267 G_A: 0.241 cycle_A: 0.085 idt_A: 0.063 D_B: 0.264 G_B: 0.270 cycle_B: 0.145 idt_B: 0.038 \n",
            "End of epoch 173 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000535 -> 0.0000515\n",
            "End of epoch 174 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000515 -> 0.0000495\n",
            "(epoch: 175, iters: 48, time: 3.663, data: 0.003) D_A: 0.248 G_A: 0.248 cycle_A: 0.240 idt_A: 0.058 D_B: 0.265 G_B: 0.248 cycle_B: 0.132 idt_B: 0.102 \n",
            "saving the model at the end of epoch 175, iters 8400\n",
            "End of epoch 175 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000495 -> 0.0000475\n",
            "End of epoch 176 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000475 -> 0.0000455\n",
            "End of epoch 177 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000455 -> 0.0000436\n",
            "(epoch: 178, iters: 4, time: 1.433, data: 0.217) D_A: 0.253 G_A: 0.256 cycle_A: 0.058 idt_A: 0.094 D_B: 0.255 G_B: 0.228 cycle_B: 0.216 idt_B: 0.027 \n",
            "End of epoch 178 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000436 -> 0.0000416\n",
            "End of epoch 179 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000416 -> 0.0000396\n",
            "(epoch: 180, iters: 8, time: 1.432, data: 0.005) D_A: 0.259 G_A: 0.240 cycle_A: 0.169 idt_A: 0.054 D_B: 0.245 G_B: 0.250 cycle_B: 0.121 idt_B: 0.069 \n",
            "saving the model at the end of epoch 180, iters 8640\n",
            "End of epoch 180 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000396 -> 0.0000376\n",
            "End of epoch 181 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000376 -> 0.0000356\n",
            "(epoch: 182, iters: 12, time: 1.427, data: 0.004) D_A: 0.268 G_A: 0.228 cycle_A: 0.056 idt_A: 0.075 D_B: 0.241 G_B: 0.257 cycle_B: 0.162 idt_B: 0.026 \n",
            "End of epoch 182 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000356 -> 0.0000337\n",
            "End of epoch 183 / 200 \t Time Taken: 64 sec\n",
            "learning rate 0.0000337 -> 0.0000317\n",
            "(epoch: 184, iters: 16, time: 4.144, data: 0.003) D_A: 0.258 G_A: 0.261 cycle_A: 0.131 idt_A: 0.061 D_B: 0.257 G_B: 0.276 cycle_B: 0.131 idt_B: 0.062 \n",
            "End of epoch 184 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000317 -> 0.0000297\n",
            "saving the model at the end of epoch 185, iters 8880\n",
            "End of epoch 185 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000297 -> 0.0000277\n",
            "(epoch: 186, iters: 20, time: 1.434, data: 0.008) D_A: 0.254 G_A: 0.250 cycle_A: 0.136 idt_A: 0.062 D_B: 0.243 G_B: 0.278 cycle_B: 0.135 idt_B: 0.062 \n",
            "End of epoch 186 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000277 -> 0.0000257\n",
            "End of epoch 187 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000257 -> 0.0000238\n",
            "(epoch: 188, iters: 24, time: 1.439, data: 0.005) D_A: 0.260 G_A: 0.246 cycle_A: 0.177 idt_A: 0.030 D_B: 0.249 G_B: 0.252 cycle_B: 0.066 idt_B: 0.076 \n",
            "End of epoch 188 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000238 -> 0.0000218\n",
            "End of epoch 189 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000218 -> 0.0000198\n",
            "(epoch: 190, iters: 28, time: 1.427, data: 0.005) D_A: 0.251 G_A: 0.253 cycle_A: 0.097 idt_A: 0.055 D_B: 0.254 G_B: 0.249 cycle_B: 0.127 idt_B: 0.046 \n",
            "saving the model at the end of epoch 190, iters 9120\n",
            "End of epoch 190 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000198 -> 0.0000178\n",
            "End of epoch 191 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000178 -> 0.0000158\n",
            "(epoch: 192, iters: 32, time: 3.875, data: 0.004) D_A: 0.247 G_A: 0.248 cycle_A: 0.102 idt_A: 0.048 D_B: 0.245 G_B: 0.245 cycle_B: 0.104 idt_B: 0.048 \n",
            "End of epoch 192 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000158 -> 0.0000139\n",
            "End of epoch 193 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000139 -> 0.0000119\n",
            "(epoch: 194, iters: 36, time: 1.433, data: 0.004) D_A: 0.256 G_A: 0.256 cycle_A: 0.092 idt_A: 0.079 D_B: 0.251 G_B: 0.252 cycle_B: 0.178 idt_B: 0.041 \n",
            "End of epoch 194 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000119 -> 0.0000099\n",
            "saving the model at the end of epoch 195, iters 9360\n",
            "End of epoch 195 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000099 -> 0.0000079\n",
            "(epoch: 196, iters: 40, time: 1.421, data: 0.004) D_A: 0.257 G_A: 0.259 cycle_A: 0.117 idt_A: 0.095 D_B: 0.250 G_B: 0.267 cycle_B: 0.215 idt_B: 0.054 \n",
            "End of epoch 196 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000079 -> 0.0000059\n",
            "End of epoch 197 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000059 -> 0.0000040\n",
            "(epoch: 198, iters: 44, time: 1.431, data: 0.004) D_A: 0.257 G_A: 0.254 cycle_A: 0.118 idt_A: 0.037 D_B: 0.262 G_B: 0.259 cycle_B: 0.086 idt_B: 0.053 \n",
            "End of epoch 198 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000040 -> 0.0000020\n",
            "End of epoch 199 / 200 \t Time Taken: 65 sec\n",
            "learning rate 0.0000020 -> 0.0000000\n",
            "(epoch: 200, iters: 48, time: 3.961, data: 0.005) D_A: 0.250 G_A: 0.263 cycle_A: 0.122 idt_A: 0.078 D_B: 0.242 G_B: 0.267 cycle_B: 0.174 idt_B: 0.055 \n",
            "saving the model at the end of epoch 200, iters 9600\n",
            "End of epoch 200 / 200 \t Time Taken: 68 sec\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     D_A â–†â–‚â–ƒâ–ƒâ–…â–ƒâ–‚â–‚â–ˆâ–‚â–„â–‚â–‚â–ƒâ–‚â–ƒâ–†â–„â–‚â–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     D_B â–ƒâ–‚â–‚â–ƒâ–ˆâ–†â–‚â–†â–ƒâ–‡â–â–†â–ƒâ–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–‚â–â–ƒâ–â–‚â–‚â–‚â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     G_A â–„â–ƒâ–ƒâ–ƒâ–…â–„â–‚â–…â–ˆâ–â–ƒâ–â–‚â–‚â–‚â–‚â–…â–„â–‚â–‚â–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     G_B â–„â–‚â–ƒâ–…â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–…â–†â–„â–‚â–„â–†â–â–â–„â–ƒâ–ƒâ–‚â–â–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–‚â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: cycle_A â–ˆâ–ƒâ–‡â–ƒâ–ƒâ–„â–‚â–„â–ƒâ–‚â–„â–‚â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: cycle_B â–„â–„â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–â–â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   idt_A â–„â–„â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–â–â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   idt_B â–ˆâ–ƒâ–†â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–„â–‚â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     D_A 0.25016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     D_B 0.24169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     G_A 0.26332\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     G_B 0.2669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: cycle_A 0.12207\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: cycle_B 0.17381\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   idt_A 0.07752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   idt_B 0.0545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcttomri\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/v3dhawk/CycleGAN-and-pix2pix/runs/37yme9h7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 216 media file(s), 216 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220511_130256-37yme9h7/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#to train from scratch\n",
        "\n",
        "!python train.py --dataroot ./datasets/dataset/ --name cttomri --model cycle_gan --use_wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to load previously trained weights , mae directory as below and then upload weights in it\n",
        "%cd /content/pytorch-CycleGAN-and-pix2pix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ58r_tdQICY",
        "outputId": "1206a251-b14c-4912-d304-12fbbe6514bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-CycleGAN-and-pix2pix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir checkpoints"
      ],
      "metadata": {
        "id": "3-Y1-38qQNbG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "seB06v68QjjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkcaFZiyASl"
      },
      "source": [
        "# Testing\n",
        "\n",
        "-   `python test.py --dataroot datasets/(dataset name)/testA --name (model name)--model test --no_dropout`\n",
        "\n",
        "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCsKkEq0yGh0",
        "outputId": "6018806f-402f-4154-8bae-6136f88543ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 1                             \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "                crop_size: 256                           \n",
            "                 dataroot: datasets/dataset/testA        \t[default: None]\n",
            "             dataset_mode: single                        \n",
            "                direction: AtoB                          \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "                     eval: False                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: False                         \t[default: None]\n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 256                           \n",
            "         max_dataset_size: inf                           \n",
            "                    model: test                          \n",
            "             model_suffix:                               \n",
            "               n_layers_D: 3                             \n",
            "                     name: cttomri                       \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \t[default: False]\n",
            "                  no_flip: False                         \n",
            "                     norm: instance                      \n",
            "                 num_test: 50                            \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: test                          \n",
            "               preprocess: resize_and_crop               \n",
            "              results_dir: ./results/                    \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "                use_wandb: True                          \t[default: False]\n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "dataset [SingleDataset] was created\n",
            "initialize network with normal\n",
            "model [TestModel] was created\n",
            "loading the model from ./checkpoints/cttomri/latest_net_G.pth\n",
            "Traceback (most recent call last):\n",
            "  File \"test.py\", line 52, in <module>\n",
            "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 88, in setup\n",
            "    self.load_networks(load_suffix)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 192, in load_networks\n",
            "    state_dict = torch.load(load_path, map_location=str(self.device))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 699, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 231, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 212, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/cttomri/latest_net_G.pth'\n"
          ]
        }
      ],
      "source": [
        "!python test.py --dataroot datasets/dataset/testA --name cttomri --model test --no_dropout --use_wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSKIPUByfiN"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mgg8raPyizq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('image path for fake image')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G3oVH9DyqLQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('image path for real image')\n",
        "plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CT CycleGAN",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-3.m74",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
